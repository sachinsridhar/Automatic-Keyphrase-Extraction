{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying heurestic approaches to identify candidate keyphrases from given chunks of text.\n",
    "\n",
    "**Three important steps will be considered in this stage:**\n",
    "1. Use Standard (+ customized, if required) Stop words lists to exclude stopwords\n",
    "2. Apply POS tagging and considering only certain POS Tags candidate keyphrases\n",
    "3. Matching pre-defined lexico-syntactic patterns (after chunking)\n",
    "\n",
    "Then, we apply tf-idf as our baseline keyphrase identfication method, over the shortlisted candidate keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the required libraries\n",
    "\n",
    "import nltk # the main NLP package; offers a set of corpora and easy interfaces to access them\n",
    "from nltk import *\n",
    "import string # We can now call on various methods to convert into large case, small case, deal with punctuation, etc.\n",
    "import itertools # We can now slice text, group by (aggregate), count and repeat "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code 1: Function to Extract Candidate Keyphrases from the given text corpus\n",
    "# Source of the code: http://bdewilde.github.io/blog/blogger/2013/10/15/friedman-corpus-1-background-and-creation/\n",
    "# Already a part of ModuleQ documentation (Article on KeyPhrase Extraction by Burton de Wilde)\n",
    "\n",
    "def lambda_unpack(f):\n",
    "    return(lambda args: f(*args))\n",
    "# Putting *args as the last item of the function allows that function to accept an arbitrary number of arguments\n",
    "\n",
    "# The piece of code below is used to set rules and define the logic for POS-based candidate keyphrase chunking\n",
    "# The function takes two arguments - the first is the piece of text that we are analyzing\n",
    "# The second argument is the \"grammar\", which is passed through the regexp parser, to consider \n",
    "# only those phrases of text which satisfy the POS Sequence specified in the grammar\n",
    "\n",
    "# This is a \"noun-phrases only\" (NP) heurestic, where only the nouns and the associated adjectives have\n",
    "# been considered objects of interest\n",
    "# This makes sense in a setting where we are only interested in studying the business priorities \n",
    "# within an e-mail, which are likely to be described by the nouns in the text\n",
    "\n",
    "def extract_candidate_chunks(text, grammar=r'KT: {(<JJ>* <NN.*>+ <IN>)? <JJ>* <NN.*>+}'):\n",
    "                                                             # We pass this POS Sequence through the Regex Parser\n",
    "    \n",
    "    punct = set(string.punctuation) # creating a set of all punctuations \n",
    "    stop_words = set(nltk.corpus.stopwords.words('english')) # creating a set of all stopwords from the library\n",
    "                                                             # in order to exclude stop words/just punctuation\n",
    "# Note: JJ = Adjective; NN = Noun (Singular); IN = Preposition or subordinating conjunction\n",
    "# Note: The '*', '+' and '?' qualifiers are greedy - they match as much text as possible\n",
    "# ab* will match 'a','ab' or 'a' followed by any number of 'b's\n",
    "# ab+ will match 'a' followed by any non-zero number of 'b's ('+' allowed for extra occurences of the last string)\n",
    "# ab? will match either 'a' or 'ab'\n",
    "\n",
    "# tokenize, POS-tag, and chunk using regular expressions\n",
    "    chunker = nltk.chunk.regexp.RegexpParser(grammar)\n",
    "  \n",
    " # nltk.chunk.regexp.ChunkRule(tag_pattern, descr)\n",
    " # A rule specifying how to add chunks to a ChunkString, using a matching tag pattern. \n",
    " # When applied to a ChunkString, it will find any substring that matches this tag pattern and that is not already \n",
    " # part of a chunk, and create a new chunk containing that substring.\n",
    "\n",
    "    tagged_sents = nltk.pos_tag_sents(nltk.word_tokenize(sent) for sent in nltk.sent_tokenize(text))\n",
    "# sent_tokenize is an nltk function to split a given text by sentence.\n",
    "# nltk.pos_tag_sents function takes on the tokens (single words) of a sentence and assigns as POS Tag to it.\n",
    "# If there are 5 sentences in the corpus, we will now have a list of 5 lists.\n",
    "\n",
    "    all_chunks = list(itertools.chain.from_iterable(nltk.chunk.tree2conlltags(chunker.parse(tagged_sent))\n",
    "                                                    for tagged_sent in tagged_sents))\n",
    "# join constituent chunk words into a single chunked phrase\n",
    "# Exclude O, becuase they represent stopwords, useless words\n",
    "# Note that all_chunks is a list of tuples in the format (word, pos, chunk)\n",
    "\n",
    "# nltk.chunk.util.tree2conlltags(t)\n",
    "# Returns a list of 3-tuples containing (word, tag, IOB-tag). \n",
    "# Convert a tree to the CoNLL IOB tag format.\n",
    "# Parameters: (Tree) â€“ The tree to be converted.\n",
    "# Return type:list(tuple)\n",
    "\n",
    "# The IOB format (short for inside, outside, beginning) is a common tagging format for tagging tokens \n",
    "# in a chunking task \n",
    "# The B- prefix before a tag indicates that the tag is the beginning of a chunk, \n",
    "# An I- prefix before a tag indicates that the tag is inside a chunk. \n",
    "# The B- tag is used only when a tag is followed by a tag of the same type without O tokens between them. \n",
    "# An O tag indicates that a token belongs to no chunk.\n",
    "\n",
    "# itertools.chain(*iterables)\n",
    "# Make an iterator that returns elements from the first iterable until it is exhausted, \n",
    "# then proceeds to the next iterable, until all of the iterables are exhausted. \n",
    "# It is used for treating consecutive sequences as a single sequence. \n",
    "# It is roughly equivalent to:\n",
    "\n",
    "# def chain(*iterables):\n",
    "#     chain('ABC', 'DEF') --> A B C D E F\n",
    "#     for it in iterables:\n",
    "#         for element in it:\n",
    "#             yield element\n",
    "\n",
    "    candidates = [' '.join(word for word, pos, chunk in group).lower()\n",
    "                  for key, group in itertools.groupby(all_chunks, lambda_unpack(lambda word,pos,chunk : chunk != 'O')) if key]\n",
    "\n",
    "    # Using the list of punctuation characters and stopwords to filter out keyphrases    \n",
    "    return [cand for cand in candidates if cand not in stop_words and not all(char in punct for char in cand)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample to understand the function line by line\n",
    "\n",
    "text = \"Where are we with respect to the presentation for MoneyGram tomorrow? \\\n",
    "Have we validated and plugged in the numbers in the deck. Don't forget - Western Union \\\n",
    "is the largest money transfer service across the world, let's be careful.\"\n",
    "grammar = r'KT: {(<JJ>* <NN.*>+ <IN>)? <JJ>* <NN.*>+}'\n",
    "\n",
    "punct = set(string.punctuation) \n",
    "stop_words = set(nltk.corpus.stopwords.words('english')) \n",
    "\n",
    "chunker = nltk.chunk.regexp.RegexpParser(grammar)\n",
    "\n",
    "tagged_sents = nltk.pos_tag_sents(nltk.word_tokenize(sent) for sent in nltk.sent_tokenize(text))\n",
    "all_chunks = list(itertools.chain.from_iterable(nltk.chunk.tree2conlltags(chunker.parse(tagged_sent))\n",
    "                                                    for tagged_sent in tagged_sents))\n",
    "    \n",
    "\n",
    "candidates = [' '.join(word for word, pos, chunk in group).lower()\n",
    "                  for key, group in itertools.groupby(all_chunks, lambda_unpack(lambda word,pos,chunk : chunk != 'O')) if key]\n",
    "#print(cand for cand in candidates if cand not in stop_words and not all(char in punct for char in cand))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('Where', 'WRB'),\n",
       "  ('are', 'VBP'),\n",
       "  ('we', 'PRP'),\n",
       "  ('with', 'IN'),\n",
       "  ('respect', 'NN'),\n",
       "  ('to', 'TO'),\n",
       "  ('the', 'DT'),\n",
       "  ('presentation', 'NN'),\n",
       "  ('for', 'IN'),\n",
       "  ('MoneyGram', 'NNP'),\n",
       "  ('tomorrow', 'NN'),\n",
       "  ('?', '.')],\n",
       " [('Have', 'VBP'),\n",
       "  ('we', 'PRP'),\n",
       "  ('validated', 'VBN'),\n",
       "  ('and', 'CC'),\n",
       "  ('plugged', 'VBN'),\n",
       "  ('in', 'IN'),\n",
       "  ('the', 'DT'),\n",
       "  ('numbers', 'NNS'),\n",
       "  ('in', 'IN'),\n",
       "  ('the', 'DT'),\n",
       "  ('deck', 'NN'),\n",
       "  ('.', '.')],\n",
       " [('Do', 'VBP'),\n",
       "  (\"n't\", 'RB'),\n",
       "  ('forget', 'VB'),\n",
       "  ('-', ':'),\n",
       "  ('Western', 'NNP'),\n",
       "  ('Union', 'NNP'),\n",
       "  ('is', 'VBZ'),\n",
       "  ('the', 'DT'),\n",
       "  ('largest', 'JJS'),\n",
       "  ('money', 'NN'),\n",
       "  ('transfer', 'NN'),\n",
       "  ('service', 'NN'),\n",
       "  ('across', 'IN'),\n",
       "  ('the', 'DT'),\n",
       "  ('world', 'NN'),\n",
       "  (',', ','),\n",
       "  ('let', 'VB'),\n",
       "  (\"'s\", 'POS'),\n",
       "  ('be', 'VB'),\n",
       "  ('careful', 'JJ'),\n",
       "  ('.', '.')]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Where', 'WRB', 'O'),\n",
       " ('are', 'VBP', 'O'),\n",
       " ('we', 'PRP', 'O'),\n",
       " ('with', 'IN', 'O'),\n",
       " ('respect', 'NN', 'B-KT'),\n",
       " ('to', 'TO', 'O'),\n",
       " ('the', 'DT', 'O'),\n",
       " ('presentation', 'NN', 'B-KT'),\n",
       " ('for', 'IN', 'I-KT'),\n",
       " ('MoneyGram', 'NNP', 'I-KT'),\n",
       " ('tomorrow', 'NN', 'I-KT'),\n",
       " ('?', '.', 'O'),\n",
       " ('Have', 'VBP', 'O'),\n",
       " ('we', 'PRP', 'O'),\n",
       " ('validated', 'VBN', 'O'),\n",
       " ('and', 'CC', 'O'),\n",
       " ('plugged', 'VBN', 'O'),\n",
       " ('in', 'IN', 'O'),\n",
       " ('the', 'DT', 'O'),\n",
       " ('numbers', 'NNS', 'B-KT'),\n",
       " ('in', 'IN', 'O'),\n",
       " ('the', 'DT', 'O'),\n",
       " ('deck', 'NN', 'B-KT'),\n",
       " ('.', '.', 'O'),\n",
       " ('Do', 'VBP', 'O'),\n",
       " (\"n't\", 'RB', 'O'),\n",
       " ('forget', 'VB', 'O'),\n",
       " ('-', ':', 'O'),\n",
       " ('Western', 'NNP', 'B-KT'),\n",
       " ('Union', 'NNP', 'I-KT'),\n",
       " ('is', 'VBZ', 'O'),\n",
       " ('the', 'DT', 'O'),\n",
       " ('largest', 'JJS', 'O'),\n",
       " ('money', 'NN', 'B-KT'),\n",
       " ('transfer', 'NN', 'I-KT'),\n",
       " ('service', 'NN', 'I-KT'),\n",
       " ('across', 'IN', 'O'),\n",
       " ('the', 'DT', 'O'),\n",
       " ('world', 'NN', 'B-KT'),\n",
       " (',', ',', 'O'),\n",
       " ('let', 'VB', 'O'),\n",
       " (\"'s\", 'POS', 'O'),\n",
       " ('be', 'VB', 'O'),\n",
       " ('careful', 'JJ', 'O'),\n",
       " ('.', '.', 'O')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['respect',\n",
       " 'presentation for moneygram tomorrow',\n",
       " 'numbers',\n",
       " 'deck',\n",
       " 'western union',\n",
       " 'money transfer service',\n",
       " 'world']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "respect\n",
      "presentation for moneygram tomorrow\n",
      "numbers\n",
      "deck\n",
      "western union\n",
      "money transfer service\n",
      "world\n"
     ]
    }
   ],
   "source": [
    "for cand in candidates:\n",
    "    if cand not in stop_words and not all(char in punct for char in cand):\n",
    "        print(cand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['brute-force method',\n",
       " 'words',\n",
       " 'phrases',\n",
       " 'document as candidate keyphrases',\n",
       " 'computational costs',\n",
       " 'fact',\n",
       " 'words',\n",
       " 'phrases',\n",
       " 'document',\n",
       " 'content',\n",
       " 'heuristics',\n",
       " 'subset',\n",
       " 'candidates',\n",
       " 'common heuristics',\n",
       " 'stop words',\n",
       " 'punctuation',\n",
       " 'words with certain parts',\n",
       " 'speech',\n",
       " 'multi-word phrases',\n",
       " 'certain pos patterns',\n",
       " 'external knowledge bases like wordnet',\n",
       " 'wikipedia',\n",
       " 'reference source of good/bad keyphrases']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_candidate_chunks(\"A brute-force method might consider all words and/or phrases in a document as candidate keyphrases. However, given computational costs and the fact that not all words and phrases in a document are equally likely to convey its content, heuristics are typically used to identify a smaller subset of better candidates. Common heuristics include removing stop words and punctuation; filtering for words with certain parts of speech or, for multi-word phrases, certain POS patterns; and using external knowledge bases like WordNet or Wikipedia as a reference source of good/bad keyphrases.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_candidate_words(text, good_tags=set(['JJ','JJR','JJS','NN','NNP','NNS','NNPS'])):\n",
    "    import itertools, nltk, string\n",
    "\n",
    "    # exclude candidates that are stop words or entirely punctuation\n",
    "    punct = set(string.punctuation)\n",
    "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "    # tokenize and POS-tag words\n",
    "    tagged_words = itertools.chain.from_iterable(nltk.pos_tag_sents(nltk.word_tokenize(sent)\n",
    "                                                                    for sent in nltk.sent_tokenize(text)))\n",
    "    # filter on certain POS tags and lowercase all words\n",
    "    candidates = [word.lower() for word, tag in tagged_words\n",
    "                  if tag in good_tags and word.lower() not in stop_words\n",
    "                  and not all(char in punct for char in word)]\n",
    "\n",
    "    return candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['brute-force',\n",
       " 'method',\n",
       " 'words',\n",
       " 'phrases',\n",
       " 'document',\n",
       " 'candidate',\n",
       " 'keyphrases',\n",
       " 'computational',\n",
       " 'costs',\n",
       " 'fact',\n",
       " 'words',\n",
       " 'phrases',\n",
       " 'document',\n",
       " 'likely',\n",
       " 'content',\n",
       " 'heuristics',\n",
       " 'smaller',\n",
       " 'subset',\n",
       " 'better',\n",
       " 'candidates',\n",
       " 'common',\n",
       " 'heuristics',\n",
       " 'stop',\n",
       " 'words',\n",
       " 'punctuation',\n",
       " 'words',\n",
       " 'certain',\n",
       " 'parts',\n",
       " 'speech',\n",
       " 'multi-word',\n",
       " 'phrases',\n",
       " 'certain',\n",
       " 'pos',\n",
       " 'patterns',\n",
       " 'external',\n",
       " 'knowledge',\n",
       " 'bases',\n",
       " 'wordnet',\n",
       " 'wikipedia',\n",
       " 'reference',\n",
       " 'source',\n",
       " 'good/bad',\n",
       " 'keyphrases']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Applying the function\n",
    "extract_candidate_words(\"A brute-force method might consider all words and/or phrases in a document as candidate keyphrases. However, given computational costs and the fact that not all words and phrases in a document are equally likely to convey its content, heuristics are typically used to identify a smaller subset of better candidates. Common heuristics include removing stop words and punctuation; filtering for words with certain parts of speech or, for multi-word phrases, certain POS patterns; and using external knowledge bases like WordNet or Wikipedia as a reference source of good/bad keyphrases.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a b c'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Illustration\n",
    "\" \".join([\"a\", \"b\", \"c\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, let us apply the tf-idf algorithm to these candidate keywords\n",
    "import gensim\n",
    "def score_keyphrases_by_tfidf(texts, candidates='chunks'):\n",
    "    import gensim, nltk\n",
    "    \n",
    "    # extract candidates from each text in texts, either chunks or words\n",
    "    if candidates == 'chunks':\n",
    "        boc_texts = [extract_candidate_chunks(text) for text in texts]\n",
    "    elif candidates == 'words':\n",
    "        boc_texts = [extract_candidate_words(text) for text in texts]\n",
    "    # make gensim dictionary and corpus\n",
    "    dictionary = gensim.corpora.Dictionary(boc_texts)\n",
    "    corpus = [dictionary.doc2bow(boc_text) for boc_text in boc_texts]\n",
    "    # transform corpus with tf*idf model\n",
    "    tfidf = gensim.models.TfidfModel(corpus)\n",
    "    corpus_tfidf = tfidf[corpus]\n",
    "    print(corpus_tfidf)\n",
    "    return corpus_tfidf, dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<gensim.interfaces.TransformedCorpus object at 0x10cb75f60>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<gensim.interfaces.TransformedCorpus at 0x10cb75f60>,\n",
       " <gensim.corpora.dictionary.Dictionary at 0x10cb755c0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_keyphrases_by_tfidf(texts)#,\"Testing four five. Testing five. Testing six.\",\"Testing six, Testing seven.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\"Testing one. Red butterfly. Castle island.\",\"Business interests, testing five.\",\"Testing five, testing six, the blue butterfly\"]\n",
    "# Task: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'corpus_tfidf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-6d29b92e70fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus_tfidf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'corpus_tfidf' is not defined"
     ]
    }
   ],
   "source": [
    "print(corpus_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
